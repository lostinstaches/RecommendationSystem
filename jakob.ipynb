{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_data, preprocess_data\n",
    "from plots import plot_train_test_data\n",
    "from helpers import split_data\n",
    "from helpers import init_MF\n",
    "from plots import plot_raw_data\n",
    "from helpers import calculate_mse\n",
    "\n",
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items: 10000, number of users: 1000\n"
     ]
    }
   ],
   "source": [
    "path_dataset = \"./data/de_data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_items_per_user, num_users_per_item = plot_raw_data(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:1059177\n",
      "Total number of nonzero elements in test data:117775\n"
     ]
    }
   ],
   "source": [
    "valid_ratings, train, test = split_data(\n",
    "    ratings, num_items_per_user, num_users_per_item, min_num_ratings=0, p_test=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1217386297173755\n"
     ]
    }
   ],
   "source": [
    "def baseline_global_mean(train, test):\n",
    "    \"\"\"baseline method: use the global mean.\"\"\"\n",
    "    global_mean = train[train.nonzero()].mean()\n",
    "    nnz_test = test[test.nonzero()].todense() \n",
    "    mse = calculate_mse(nnz_test, global_mean)\n",
    "    rmse = np.sqrt(mse / nnz_test.shape[1])\n",
    "    return np.squeeze(rmse).item()\n",
    "print(\"RMSE global: {}\".format(baseline_global_mean(train, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE user-mean: 1.0319099240955814\n"
     ]
    }
   ],
   "source": [
    "def baseline_user_mean(train, test):\n",
    "    \"\"\"baseline method: use the user means as the prediction.\"\"\"\n",
    "    mse = 0\n",
    "    _, num_users = train.shape\n",
    "    for user in range(num_users):\n",
    "        user_ratings_train = train[:, user]\n",
    "        nnz_train = user_ratings_train[user_ratings_train.nonzero()]\n",
    "        if nnz_train.shape[0] == 0:\n",
    "            continue\n",
    "        user_mean = nnz_train.mean()            \n",
    "        user_ratings_test = test[:, user]\n",
    "        nnz_test = user_ratings_test[user_ratings_test.nonzero()].todense()\n",
    "        mse += calculate_mse(nnz_test, user_mean)\n",
    "    return np.sqrt(1.0 * mse / test.nnz).item()\n",
    "print(\"RMSE user-mean: {}\".format(baseline_user_mean(train, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE item-mean: 1.0959075588942098\n"
     ]
    }
   ],
   "source": [
    "def baseline_item_mean(train, test):\n",
    "    \"\"\"baseline method: use item means as the prediction.\"\"\"\n",
    "    return baseline_user_mean(train.T, test.T)\n",
    "print(\"RMSE item-mean: {}\".format(baseline_item_mean(train, test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline estimate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SGD...\n",
      "iter: 0, RMSE on training set: 0.9994629076738408.\n",
      "iter: 1, RMSE on training set: 0.9956075486578966.\n",
      "iter: 2, RMSE on training set: 0.99370587331951.\n",
      "iter: 3, RMSE on training set: 0.9930551822462327.\n",
      "iter: 4, RMSE on training set: 0.9924684505310238.\n",
      "iter: 5, RMSE on training set: 0.9920580299985728.\n",
      "iter: 6, RMSE on training set: 0.9915115565597591.\n",
      "iter: 7, RMSE on training set: 0.9913468214722916.\n",
      "iter: 8, RMSE on training set: 0.9911768392951089.\n",
      "iter: 9, RMSE on training set: 0.9910120783857594.\n",
      "RMSE on test data: 1.001507976095007.\n"
     ]
    }
   ],
   "source": [
    "def compute_error_baseline_estimate(data, \n",
    "                                    nz, \n",
    "                                    user_bias, \n",
    "                                    item_bias, \n",
    "                                    global_bias,\n",
    "                                    lambda_user_bias,\n",
    "                                    lambda_item_bias):\n",
    "    mse = 0\n",
    "    for item, user in nz:\n",
    "        prediction = global_bias + item_bias[item] + user_bias[user]\n",
    "        mse += (data[item, user] - prediction) ** 2\n",
    "    reg = lambda_user_bias * user_bias.T.dot(user_bias) + lambda_item_bias * item_bias.T.dot(item_bias)\n",
    "    mse += reg\n",
    "    return np.sqrt(mse / len(nz))\n",
    "\n",
    "\n",
    "# See 2.1 http://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf\n",
    "def baseline_estimate(train,\n",
    "                      test,\n",
    "                      gamma=0.02, \n",
    "                      num_features=20,\n",
    "                      lambda_user_bias=0.1,\n",
    "                      lambda_item_bias=0.1,\n",
    "                      num_epochs=10):\n",
    "    \n",
    "    global_bias = train[train.nonzero()].mean()\n",
    "    \n",
    "    # He init\n",
    "    user_bias = np.random.normal(scale=np.sqrt(2/num_features), size=train.shape[1])\n",
    "    item_bias = np.random.normal(scale=np.sqrt(2/num_features), size=train.shape[0])\n",
    "    \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"Starting SGD...\")\n",
    "    for it in range(num_epochs):\n",
    "        np.random.shuffle(nz_train)\n",
    "        gamma /= 1.2\n",
    "        for d, n in nz_train:\n",
    "            prediction = global_bias + user_bias[n] + item_bias[d]\n",
    "            error = train[d, n] - prediction\n",
    "            # Update variables\n",
    "            user_bias[n] +=  gamma * (error - lambda_user_bias * user_bias[n])\n",
    "            item_bias[d] +=  gamma * (error - lambda_item_bias * item_bias[d])\n",
    "        rmse = compute_error_baseline_estimate(train, \n",
    "                                               nz_train, \n",
    "                                               user_bias, \n",
    "                                               item_bias, \n",
    "                                               global_bias,\n",
    "                                               lambda_user_bias,\n",
    "                                               lambda_item_bias)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse))\n",
    "        \n",
    "    rmse = compute_error_baseline_estimate(test, \n",
    "                                           nz_test, \n",
    "                                           user_bias, \n",
    "                                           item_bias,\n",
    "                                           global_bias,\n",
    "                                           lambda_user_bias,\n",
    "                                           lambda_item_bias)\n",
    "    print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    \n",
    "    return user_bias, item_bias, global_bias \n",
    "\n",
    "user_bias, item_bias, global_bias = baseline_estimate(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_submission_baseline_estimate\n",
    "create_submission_baseline_estimate(user_bias, item_bias, global_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import build_index_groups\n",
    "\n",
    "nz_train, nz_item_userindices, nz_user_itemindices = build_index_groups(train)\n",
    "\n",
    "num_items, num_users = train.shape\n",
    "for user, items in nz_user_itemindices:\n",
    "    user_means[user] = train[items, user].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~10 min to compute!\n",
    "def construct_similarity_matrix(data, nz_user_userindices):\n",
    "    num_items, num_users = data.shape\n",
    "    print(\"Constructing sim matrix\")\n",
    "    similarity_matrix = np.zeros((num_users, num_users))\n",
    "    for i, items_i in nz_user_itemindices:\n",
    "        print(i)\n",
    "        for j, items_j in nz_user_itemindices:\n",
    "            if i == j:\n",
    "                continue\n",
    "                            \n",
    "            item_intersection = np.intersect1d(items_i, items_j)\n",
    "            \n",
    "            if len(item_intersection) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate pearson coefficient, could might as well have used np.corrcoef\n",
    "            # but calculatig it yourself is so much more fun!\n",
    "            \n",
    "            r_i = data[item_intersection, i].todense()\n",
    "            r_i = r_i -  r_i.mean()\n",
    "            r_j = data[item_intersection, j].todense()\n",
    "            r_j = r_j -  r_j.mean()\n",
    "     \n",
    "            numerator = r_i.T @ r_j\n",
    "\n",
    "            denominator = np.sqrt(r_i.T.dot(r_i)) * np.sqrt(r_j.T.dot(r_j))\n",
    "            similarity_matrix[i, j] = numerator / denominator if denominator > 0 else 0       \n",
    "            \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_s_matrix(train, nz_train, user_means):\n",
    "    s_matrix = sp.lil_matrix(train.shape)\n",
    "    #nz_row, nz_col = train.nonzero()\n",
    "    #nz_train = list(zip(nz_row, nz_col))\n",
    "    print(\"Constructing S matrix...\")\n",
    "    for d, n in nz_train:\n",
    "        s_matrix[d, n] = train[d, n] - user_means[n]\n",
    "    return s_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_knn(similarity_matrix, s_matrix, test, k, nz_item_userindices, similarity_treshold=0.1):\n",
    "    mse = 0\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    iters = 0\n",
    "    print(\"Starting evaluation..\")\n",
    "    for d, n in nz_test:\n",
    "        iters += 1\n",
    "        if iters % 10000 == 0:\n",
    "            print(iters)\n",
    "            \n",
    "        # Get index of k most similar users\n",
    "        # Set all similarityis not in d_users to zero -> argsort -> take top k.\n",
    "        _, d_users = nz_item_userindices[d]\n",
    "        d_users_similarity = similarity_matrix[n, :].copy()\n",
    "        kn_index = np.zeros(similarity_matrix.shape[1])\n",
    "        kn_index[d_users] = similarity_matrix[n, d_users]\n",
    "        kn_index = kn_index.argsort()[-k:][::-1]\n",
    "\n",
    "        kn_similarity = kn_similarity[n, kn_index] \n",
    "        \n",
    "        kn_mean_centered_rating = s_matrix[d, kn_index].todense()\n",
    "        kn_mean_centered_rating = np.squeeze(np.asarray(kn_mean_centered_rating))\n",
    "        \n",
    "        # Remove users with low similarity as described, heuristic desribed in 2.3.1\n",
    "        kn_mean_centered_rating = kn_mean_centered_rating[kn_similarity > similarity_treshold]\n",
    "        kn_similarity = kn_similarity[kn_similarity > similarity_treshold]\n",
    "\n",
    "        # Compute prediction\n",
    "        numerator = kn_mean_centered_rating.dot(kn_similarity)\n",
    "        denominator = np.sum(np.abs(similarity_matrix[n, kn_index]))\n",
    "        \n",
    "        prediction = numerator / denominator\n",
    "    \n",
    "        user_mean = user_means[n]\n",
    "        prediction = user_mean + prediction\n",
    "        \n",
    "        # Compute error\n",
    "        mse += (test[d, n] - prediction) ** 2\n",
    "        \n",
    "    return np.sqrt(mse / len(nz_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = construct_similarity_matrix(train, nz_user_itemindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-specific KNN\n",
    "def KNN(similarity_matrix, train, test, k, nz_train, nz_item_userindices, nz_user_itemindices, user_means):\n",
    "    #nz_train, nz_item_userindices, nz_user_itemindices = build_index_groups(train)\n",
    "    #similarity_matrix = construct_similarity_matrix(train, nz_user_itemindices)\n",
    "    s_matrix = construct_s_matrix(train, nz_train, user_means)\n",
    "    print(evaluate_knn(similarity_matrix, s_matrix, test, k, nz_item_userindices))\n",
    "    \n",
    "KNN(similarity_matrix,  train, test, 40, nz_train, nz_item_userindices, nz_user_itemindices, user_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "model = NearestNeighbors(n_neighbors=25, metric='correlation')\n",
    "model.fit(train.todense())\n",
    "distances, indices = model.kneighbors(test.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71805531 0.72333828 0.72889909 0.73265069 0.74629688 0.76158883\n",
      " 0.7670455  0.7752808  0.77637654 0.78435691 0.79026452 0.79066985\n",
      " 0.79232536 0.79292786 0.79354054 0.79468498 0.80946411 0.81175635\n",
      " 0.81250677 0.81265502 0.81674545 0.81690947 0.81691794 0.81733007\n",
      " 0.81753813]\n",
      "[6494 4381 4031 2692 5496 4681 4714 1793 6651  833 9760 6695 9594 7485\n",
      " 1324 3891 9864 6192 6124 2376 2843 5027 7361  510 9852]\n"
     ]
    }
   ],
   "source": [
    "print(distances[0])\n",
    "print(indices[0])\n",
    "#print(distances.shape)\n",
    "#print(indices.shape)\n",
    "\n",
    "#for d, n in nz_test[:20000]:\n",
    "#    k_near = distances[n]\n",
    "#    \n",
    "#    numerator = np.sum(sim * s)\n",
    "#    denominator = np.sum(np.abs(similarity_matrix[n, k_similar_index]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization baseline latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_baseline(data, user_features, item_features, nz):\n",
    "    mse = 0   \n",
    "    for row, col in nz:\n",
    "        p = user_features[:, col]\n",
    "        q = item_features[:, row]\n",
    "        prediction = p.T.dot(q)\n",
    "        mse += (data[row, col] - prediction)**2\n",
    "    return np.sqrt(1.0 * mse / len(nz))\n",
    "\n",
    "def matrix_factorization_SGD_baseline(train, \n",
    "                             test, \n",
    "                             num_features=20, \n",
    "                             gamma=0.05, \n",
    "                             lambda_user=0.1, \n",
    "                             lambda_item=0.7, \n",
    "                             num_epochs=10):     \n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "\n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "\n",
    "        for d, n in nz_train:\n",
    "\n",
    "            q = item_features[:, d]\n",
    "            p = user_features[:, n]\n",
    "            prediction = p.T.dot(q)  \n",
    "            \n",
    "            error = train[d, n] - prediction\n",
    "            \n",
    "            # Update variables\n",
    "            item_features[:, d] += gamma * (error * p - lambda_item * q)\n",
    "            user_features[:, n] += gamma * (error * q - lambda_user * p)\n",
    "     \n",
    "        rmse = compute_error_v1(train, user_features, item_features, nz_train)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse)) \n",
    "        rmse = compute_error_v1(test, user_features, item_features, nz_test)\n",
    "        print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    return item_features, user_features\n",
    "        \n",
    "item_features, user_features = matrix_factorization_SGD_baseline(train, \n",
    "                                                           test, \n",
    "                                                           gamma=0.013, \n",
    "                                                           lambda_user=0.1,\n",
    "                                                           lambda_item=0.5,\n",
    "                                                           num_epochs=5,\n",
    "                                                           num_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_biased(data, user_features, item_features, nz, user_bias, item_bias, global_mean):\n",
    "    mse = 0   \n",
    "    for d, n in nz:\n",
    "        p = user_features[:, n]\n",
    "        q = item_features[:, d]\n",
    "        base = global_mean + user_bias[n] + item_bias[d]\n",
    "        prediction = base + p.T.dot(q)\n",
    "        mse += (data[d, n] - prediction)**2\n",
    "    return np.sqrt(1.0 * mse / len(nz))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def matrix_factorization_SGD_biased(train, \n",
    "                             test, \n",
    "                             num_features=20, \n",
    "                             gamma=0.05, \n",
    "                             lambda_user_bias = 0.1,\n",
    "                             lambda_item_bias = 0.1,\n",
    "                             lambda_user=0.1, \n",
    "                             lambda_item=0.7, \n",
    "                             num_epochs=10):    \n",
    "    \"\"\"matrix factorization by SGD.\"\"\"\n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    global_mean = train[train.nonzero()].mean()\n",
    "    user_bias = np.random.normal(scale=np.sqrt(2/num_features), size=train.shape[1])\n",
    "    item_bias = np.random.normal(scale=np.sqrt(2/num_features), size=train.shape[0])\n",
    "    \n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "\n",
    "        for d, n in nz_train:\n",
    "\n",
    "            q = item_features[:, d]\n",
    "            p = user_features[:, n]\n",
    "            base_prediction = global_mean + item_bias[d] + user_bias[n]\n",
    "            prediction =  base_prediction + p.T.dot(q)\n",
    "            error = train[d, n] - prediction\n",
    "            \n",
    "            # Update variables\n",
    "            user_bias[n] +=  gamma * (error - lambda_user_bias * user_bias[n])\n",
    "            item_bias[d] +=  gamma * (error - lambda_item_bias * item_bias[d])\n",
    "            item_features[:, d] += gamma * ((error * p - lambda_item * q))\n",
    "            user_features[:, n] += gamma * ((error * q - lambda_user * p))\n",
    "     \n",
    "        rmse = compute_error_v2(train, user_features, item_features, nz_train, user_bias, item_bias, global_mean)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse)) # \n",
    "        rmse = compute_error_v2(test, user_features, item_features, nz_test, user_bias, item_bias, global_mean)\n",
    "        print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    return item_features, user_features, user_bias, item_bias, global_mean\n",
    "\n",
    "item_features, user_features, user_bias, item_bias, global_bias = matrix_factorization_SGD_biased(\n",
    "    train, \n",
    "    test, \n",
    "    gamma=0.02, \n",
    "    num_epochs=20,\n",
    "    num_features=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization SVD++ (VERY SLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_v3(data, \n",
    "                     user_features, \n",
    "                     item_features, \n",
    "                     nz, \n",
    "                     user_bias, \n",
    "                     item_bias, \n",
    "                     global_mean, \n",
    "                     yj,\n",
    "                     nnz_user):\n",
    "    mse = 0   \n",
    "    feedbacks = []\n",
    "    for user in range(user_features.shape[1]):\n",
    "        nz_row, _ = nnz_user[user]\n",
    "        implicit_feedback = np.sum(yj[nz_row, :], axis=0)\n",
    "        if len(nz_row) > 0:\n",
    "            implicit_feedback /= np.sqrt(len(nz_row))\n",
    "        feedbacks.append(implicit_feedback)\n",
    "    \n",
    "    for row, col in nz:\n",
    "        user = col\n",
    "        implicit_feedback = feedbacks[user]\n",
    "        p = user_features[:, col] + implicit_feedback\n",
    "        q = item_features[:, row]\n",
    "        base = global_mean + user_bias[col] + item_bias[row]\n",
    "        prediction = base + q.dot(p.T)\n",
    "        mse += (data[row, col] - prediction)**2\n",
    "    return np.sqrt(1.0 * mse / len(nz))\n",
    "\n",
    "\n",
    "\n",
    "# SVD++\n",
    "def matrix_factorization_SGD_v3(train, \n",
    "                             test, \n",
    "                             num_features=20, \n",
    "                             gamma=0.05, \n",
    "                             lambda_user=0.1, \n",
    "                             lambda_item=0.7,\n",
    "                             lambda_yj=0.1,\n",
    "                             lambda_user_bias=0.1,\n",
    "                             lambda_item_bias=0.1,\n",
    "                             num_epochs=5):\n",
    "    \n",
    "    \n",
    "    nz_train, nz_item_userindices, nz_user_itemindices = build_index_groups(train)\n",
    "        \n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(988)\n",
    "\n",
    "    # init matrix\n",
    "    user_features, item_features = init_MF(train, num_features)\n",
    "    \n",
    "    # find the non-zero ratings indices \n",
    "    nz_row, nz_col = train.nonzero()\n",
    "    nz_train = list(zip(nz_row, nz_col))\n",
    "    nz_row, nz_col = test.nonzero()\n",
    "    nz_test = list(zip(nz_row, nz_col))\n",
    "    \n",
    "    global_mean = train[train.nonzero()].mean()\n",
    "    user_bias = np.random.normal(scale=0.00, size=train.shape[1])\n",
    "    item_bias = np.random.normal(scale=0.00, size=train.shape[0])\n",
    "    \n",
    "    yj = np.random.normal(scale=0.1, size=(train.shape[0], num_features))\n",
    "    \n",
    "    nnz_users_train = []\n",
    "    for user in range(train.shape[1]):\n",
    "        nnz_users_train.append(train[:, user].nonzero())\n",
    "    \n",
    "    \n",
    "    print(\"learn the matrix factorization using SGD...\")\n",
    "    for it in range(num_epochs):        \n",
    "        # shuffle the training rating indices\n",
    "        np.random.shuffle(nz_train)\n",
    "        \n",
    "        # decrease step size\n",
    "        gamma /= 1.2\n",
    "    \n",
    "        iters = 0\n",
    "        \n",
    "        for d, n in nz_train:\n",
    "            iters += 1\n",
    "            if iters % 3000 == 0:\n",
    "                print(iters)\n",
    "            \n",
    "            # Calculate implicit feedback, expensive!\n",
    "            nz_row, _ = nnz_users_train[n]\n",
    "            lSqrt = np.sqrt(len(nz_row))\n",
    "            implicit_feedback = np.sum(yj[nz_row, :], axis=0)\n",
    "            implicit_feedback /= lSqrt\n",
    "            \n",
    "            q = item_features[:, d]\n",
    "            p = user_features[:, n] + implicit_feedback\n",
    "            base_prediction = global_mean + item_bias[d] + user_bias[n]\n",
    "            prediction = base_prediction + p.T.dot(q)\n",
    "            error = train[d, n] - prediction \n",
    "            \n",
    "            \n",
    "            # Update variables\n",
    "            user_bias[n] +=  gamma * (error - lambda_user_bias * user_bias[n])\n",
    "            item_bias[d] +=  gamma * (error - lambda_item_bias * item_bias[d])\n",
    "            \n",
    "            \n",
    "            # Update implicit feedback, expensive!\n",
    "            yj[nz_row, :] += gamma * (error * q / lSqrt - lambda_yj * yj[nz_row, :])\n",
    "            \n",
    "            item_features[:, d] += gamma * ((error * p - lambda_item * q))\n",
    "            user_features[:, n] += gamma * ((error * q - lambda_user * p)) \n",
    "     \n",
    "        rmse = compute_error_v3(train, user_features, item_features, nz_train, user_bias, item_bias, global_mean, yj, nnz_users_train)\n",
    "        print(\"iter: {}, RMSE on training set: {}.\".format(it, rmse)) \n",
    "        rmse = compute_error_v3(test, user_features, item_features, nz_test, user_bias, item_bias, global_mean, yj, nnz_users_train)\n",
    "        print(\"RMSE on test data: {}.\".format(rmse))\n",
    "    return item_features, user_features, user_bias, item_bias, global_mean\n",
    "\n",
    " \n",
    "# trying to beat : 0.9390441348636658\n",
    "item_features, user_features, user_bias, item_bias, global_mean = matrix_factorization_SGD_v3(train, \n",
    "                                                                                              test, \n",
    "                                                                                              gamma=0.02, \n",
    "                                                                                              num_features=17, \n",
    "                                                                                              lambda_yj=0.001, \n",
    "                                                                                              lambda_user=0.1, \n",
    "                                                                                              lambda_item=0.7, \n",
    "                                                                                              lambda_user_bias=0.1, \n",
    "                                                                                              lambda_item_bias=0.1) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
